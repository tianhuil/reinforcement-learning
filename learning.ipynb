{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pdb on\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "![[ -d \"reinforcement_learning\" ]] && pushd reinforcement_learning; git pull; popd\n",
    "![[ ! -d \"reinforcement_learning\" ]] && git clone https://github.com/tianhuil/reinforcement_learning.git\n",
    "!pushd reinforcement_learning; git show HEAD; popd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from stable_baselines3 import A2C\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from reinforcement_learning.src.logistics import Logistics\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "env = Logistics(n_rows=3, n_cols=2, palette_types=1, prob_loading = 0.1, prob_unloading = 0.2, n_steps=100)\n",
    "check_env(env)\n",
    "log_dir = \"/tmp/gym/{}\".format(int(time.time()))\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "model = A2C(\"MultiInputPolicy\", env, verbose=1, tensorboard_log=\"/tmp/tb/\").learn(200_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir /tmp/tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "\n",
    "# Instantiate the env\n",
    "vec_env = make_vec_env(Logistics, n_envs=1, env_kwargs=dict())\n",
    "\n",
    "obs = vec_env.reset()\n",
    "n_steps = 50\n",
    "for step in range(n_steps):\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    print(f\"Step {step + 1}\")\n",
    "    print(\"Action: \", action)\n",
    "    obs, reward, done, info = vec_env.step(action)\n",
    "    print(\"reward=\", reward, \"done=\", done)\n",
    "    vec_env.render()\n",
    "    if done:\n",
    "        # Note that the VecEnv resets automatically\n",
    "        # when a done signal is encountered\n",
    "        print(\"Goal reached!\", \"reward=\", reward)\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
